1. pca 는 principle component analysis

2. data distance 
유클리디언 디스턴스 = 점과 점 사이의 거리 공식
루트(i=1..n, (xi - yi)^2)

cosine similarity 
cosine 세타 = (x 내적 y) / ||x|| ||y||

3. k-means 알고리즘 (k 개의 그룹의 평균(==중심점) 이 존재) - 비지도학습
 1. 임의의 k 개의 중심점을 배치(중심점 간 거리는 멀도록)
 2. 각 데이터들을 가장 가까운 중심점으로 할당
 3. 형성된 군집 내에서 각 군집들의 중심점들을 모두 재생성 및 가장 가까운 중심점으로 데이터 재할당
 4. 중심점이 업데이트되지 않을 때까지 2,3 번 반복

4. apriori 알고리즘 (연관 규칙 분석)

  1. support = 전체 구매 물건 중 관심있는 물건 구매 건수의 비율
  support (a, b) = freq(a, b) / N -> 전체 물건 구매 중, a 와 b 를 동시에 구매한 비율

  * support (a, b) == support(b, a)

  * minimum support == 아이템이 최소 n 번 이상 구매되는 것.

  2. confidence = 특정 물건 구매 기준으로, 또 다른 물건을 같이 구매한 비율 -> 특정 물건 기준 다른 물건과의 신뢰 확인 
  confidence(a, b) = freq(a, b) / freq(a)

  * confidence(a, b) != confidence(b, a)

  3. lift(a, b) = a 를 사는 것과 b 를 사는 것 중 a 와 b 를 동시에 구매하는 비율

ex) size k = n -> 아이템 구매 개수 기준 

5. recommendation system
  1. content-based filtering = 기존 사용자의 content 기록을 분석하고, 새로운 비슷한 content 가 나왔을 때 이를 사용자에게 추천하는 것.
  2. collaborate filtering = 나와 성향이 비슷한 사람들에 대해 추천 알고리즘 작성
 - memory based approaches = 과거에 좋아했던 것들을 미래에도 좋아할 것이라는 가정하에 사용자의 과거 평가 기록에 기반하여 content 추천
 - model based approaches = 전체 content 소비에 대한 패턴을 분석하여 하나의 모델을 뽑아내고, 이에 기반하여 content 추천
  
코사인 시밀러리티 = 얼마나 유사한 사용자 혹은 상품인지를 구할 때 사용(값이 클수록 유사하다고 판단)
rui = 사용자 u 가 상품 i 에 매긴 평점
cosine_sim(u, v) = (rui rvi 곱의 합) / 루트(rui 제곱의 합) * 루트(rvi 제곱의 합)

ex) 두 사용자의 유사도를 비교할 때, 우선 두 사용자 모두 평가한 항목을 추리고, 거기에 cosine_sim 식을 적용. 

Why cosine similarity
선호도의 절대값을 비교할 때는 유클리디언 디스턴스가 좋지만, 선호 경향성을 파악할 때는 코사인 시밀러리티가 적합하다.

Matrix Factorization == 기존의 matrix 를 사용자를 설명하는 매트릭스와 컨텐츠를 설명하는 매트릭스로 분리하는 것으로, 파일 용량이 작아져서 데이터 저장량을 감소시킬 수 있다.




